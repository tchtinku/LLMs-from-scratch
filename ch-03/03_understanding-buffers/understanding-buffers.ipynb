{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding PyTorch Buffers\n",
    "\n",
    "###### In essence, PyTorch buffers are tensor attributes associated with a PyTorch module or model similar to parameters, but unlike parameters, buffers are not updated during training.\n",
    "\n",
    "###### Buffers in PyTorch are particularly useful when dealing with GPU computations, as they need to be transferred between devices (like from CPU to GPU) alongside the model's parameters. Unlike parameters, buffers do not require gradient computation, but they still need to be on the correct device to ensure that all computations are performed correctly.\n",
    "\n",
    "###### In chapter 3, we use PyTorch buffers via self.register_buffer, which is only briefly explained in the book. Since the concept and purpose are not immediately clear, this code notebook offers a longer explanation with a hands-on example.\n",
    "\n",
    "##### An example without buffers\n",
    "###### Suppose we have the following code, which is based on code from chapter 3. This version has been modified to exclude buffers. It implements the causal self-attention mechanism used in LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithoutBuffers(nn.Module):\n",
    "   \n",
    "   def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "       super().__init__()\n",
    "       self.d_out = d_out\n",
    "       self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "       self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "       self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "       self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "   def forward(self, x):\n",
    "       b, num_tokens, d_in = x.shape\n",
    "       keys = self.W_key\n",
    "       queries = self.W_query\n",
    "       values = self.W_value(x)\n",
    "\n",
    "       attn_scores = queries @ keystranspose(1, 2)\n",
    "       attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "       )\n",
    "       attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "       context_vec = attn_weights @ values\n",
    "       return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can initialize and run the module as follows on some example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "      [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "with torch.no_grad()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
