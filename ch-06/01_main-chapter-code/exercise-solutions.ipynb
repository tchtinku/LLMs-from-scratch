{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Chapter 6 Exercise solutions\n",
    "##### Exercise 6.1: Increasing the context length\n",
    "###### We can pad the inputs to the maximum number of tokens the model supports by setting the max length to 1024:\n",
    "\n",
    "###### max_length = 1024\n",
    "\n",
    "###### train_dataset = SpamDataset(base_path / \"train.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "###### val_dataset = SpamDataset(base_path / \"validation.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "###### test_dataset = SpamDataset(base_path / \"test.csv\", max_length=max_length, tokenizer=tokenizer)\n",
    "###### or, equivalently, we can define the max_length via:\n",
    "\n",
    "###### max_length = model.pos_emb.weight.shape[0]\n",
    "###### or\n",
    "\n",
    "###### max_length = BASE_CONFIG[\"context_length\"]\n",
    "###### For convenience, you can run this experiment via\n",
    "\n",
    "###### python additional-experiments.py --context_length \"model_context_length\"\n",
    "###### using the code in the ../02_bonus_additional-experiments folder, which results in a substantially worse test accuracy of 78.33% (versus the 95.67% in the main chapter).\n",
    "\n",
    "###### Exercise 6.2: Finetuning the whole model\n",
    "###### Instead of finetuning just the final transformer block, we can finetune the entire model by removing the following lines from the code:\n",
    "\n",
    "###### for param in model.parameters():\n",
    "######    param.requires_grad = False\n",
    "###### For convenience, you can run this experiment via\n",
    "\n",
    "###### python additional-experiments.py --trainable_layers all\n",
    "###### using the code in the ../02_bonus_additional-experiments folder, which results in a 1% improved test accuracy of 96.67% (versus the 95.67% in the main chapter).\n",
    "\n",
    "###### Exercise 6.3: Finetuning the first versus last token\n",
    "###### Rather than finetuning the last output token, we can finetune the first output token by changing\n",
    "\n",
    "###### model(input_batch)[:, -1, :]\n",
    "###### to\n",
    "\n",
    "###### model(input_batch)[:, 0, :]\n",
    "###### everywhere in the code.\n",
    "\n",
    "###### For convenience, you can run this experiment via\n",
    "\n",
    "###### python additional-experiments.py --trainable_token first\n",
    "###### using the code in the ../02_bonus_additional-experiments folder, which results in a substantially worse test accuracy of 75.00% (versus the 95.67% in the main chapter)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
