{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 5: Pretraining on Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"matplotlib\",\n",
    "    \"numpy\",\n",
    "    \"tiktoken\",\n",
    "    \"torch\",\n",
    "    \"tensorflow\"  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In this chapter, we implement the training loop and code for basic model evaluation to pretrain an LLM\n",
    "###### At the end of this chapter, we also load openly available pretrained weights from OpenAI into our model\n",
    "\n",
    "###### The topics covered in this chapter are shown below\n",
    "\n",
    "#### 5.1 Evaluating generative text models\n",
    "###### We start this section with a brief recap of initializing a GPT model using the code from the previous chapter\n",
    "###### Then, we discuss basic evaluation metrics for LLMs\n",
    "###### Lastly, in this section, we apply these evaluation metrics to a training and validation dataset\n",
    "\n",
    "##### 5.1.1 Using GPT to generate text\n",
    "###### We initialize a GPT model using the code from the previous chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 256,    # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"n_heads\": 12,            # Number of attention heads\n",
    "    \"n_layers\": 12,           # Number of layers\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();   # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
    "###### Modern LLMs also don't use bias vectors in the nn.Linear layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting \"qkv_bias\": False\n",
    "###### We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "######       -> This is so that more readers will be able to follow and execute the code examples on their laptop computer\n",
    "######       -> However, please feel free to increase the context_length to 1024 tokens (this would not require any code changes)\n",
    "######       -> We will also load a model with a 1024 context_length later from pretrained weights\n",
    "###### Next, we use the generate_text_simple function from the previous chapter to generate text\n",
    "###### In addition, we define two convenience functions, text_to_token_ids and token_ids_to_text, for converting between token and text representations that we use throughout this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M(\"context_length\")\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see above, the model does not produce good text because it has not been trained yet\n",
    "###### How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "###### The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "###### The next chapters on finetuning LLMs will also introduce additional ways to measure model quality\n",
    "\n",
    "#### 5.1.2 Calculating the text generation loss: cross-entropy and perplexity\n",
    "###### Suppose we have an inputs tensor containing the token IDs for 2 training examples (rows)\n",
    "###### Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate\n",
    "###### Notice that the targets are the inputs shifted by 1 position, as explained in chapter 2 when we implemented the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],  # [ \"Every effort moves\",\n",
    "                        [40, 1107, 588]])    # I really like\"],\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],  # [ \"Every effort moves\",\n",
    "                        [1107, 588, 11311]])    # I really like\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n",
    "###### Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "###### Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)   # Probability of each token in Vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The figure below, using a very small vocabulary for illustration purposes, outlines how we convert the probability scores back into text, which we discussed at the end of the previous chapter\n",
    "###### As discussed in the previous chapter, we can apply the argmax function to convert the probability scores into predicted token IDs\n",
    "###### The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token\n",
    "###### Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
