{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving Instruction-Data Via Reflection-Tuning Using GPT-4\n",
    "###### This notebook uses OpenAI's GPT-4 API to implement the dataset refinement process from the Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning paper\n",
    "###### In the original paper, the researchers refined the Alpaca and WizardLM instruction-finetuning datasets; in this notebook, we refine the instruction-dataset used in chapter 7 (however, since it has the same format as Alpaca, the same code works with the Alpaca dataset as well)\n",
    "\n",
    "###### The expected dataset format is as follows:\n",
    "\n",
    "###### {\n",
    "######        \"instruction\": \"Edit the following sentence for grammar.\",\n",
    "######        \"input\": \"He go to the park every day.\",\n",
    "######        \"output\": \"He goes to the park every day.\"\n",
    "######    },\n",
    "######    {\n",
    "######        \"instruction\": \"Convert 45 kilometers to meters.\",\n",
    "######        \"input\": \"\",\n",
    "######        \"output\": \"45 kilometers is 45000 meters.\"\n",
    "######    },\n",
    "###### Please note that this notebook reproduces the approach from the paper in which the authors used the GPT API to enhance existing datasets. However, it's important to be aware that GPT API-generated data may not be used to develop models that compete with OpenAI, as specified in the OpenAI Terms of Use: \"What you cannot do... Use Output to develop models that compete with OpenAI.\" You can find a relevant discussion here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"openai\",  # OpenAI API\n",
    "    \"tqdm\",    # Progress bar\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test OpenAI API\n",
    "###### First, let's test if the OpenAI API is correctly set up\n",
    "###### If you don't have an account yet, you need to create one at https://platform.openai.com/\n",
    "###### Note that you will also have to transfer some funds to your account as the GPT-4 API is not free (see https://platform.openai.com/settings/organization/billing/overview)\n",
    "###### Running the code exactly as it appears in this notebook costs about $0.03 (3 cents) with GPT-4o-mini as of this writing\n",
    "###### Applying the two methodologies above to all 1100 entries in the chapter 7 instruction dataset costs about $0.60 (60 cents)\n",
    "###### First, we need to provide our OpenAI API secret key, which can be found at https://platform.openai.com/api-keys\n",
    "###### Make sure not to share this key with anyone\n",
    "###### Add this secret key (\"sk-...\") to the config.json file in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load API key from a JSON file.\n",
    "# Make sure to replace \"sk-...\" with your actual API key from https://platform.openai.com/api-keys\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First, let's try the API with a simple example to make sure it works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-4o-mini\", system_prompt=None):\n",
    "    # Define the system message if a system_prompt is provided\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # Add the user prompt to the messages\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Call the API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "    \n",
    "    # Return the model's response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = \"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON Entries\n",
    "###### Next, let's load and process the instruction dataset\n",
    "###### Here, we assume that we saved the test dataset and the model responses as a JSON file that we can load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "json_file = Path(\"..\") / \"01_main-chapter-code\" / \"instruction-data.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's print one of the dataset entries to see its structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pp as pprint\n",
    "\n",
    "pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve Instructions\n",
    "###### The Reflection-Tuning authors shared two approaches: (1) improving the instructions and (2) improving the responses\n",
    "###### Let's begin by improving the instructions in a given dataset\n",
    "###### Below is a small utility function from the Reflection-Tuning repository to format the inputs to the GPT-4 model for this dataset refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def instr_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of a given instruction. \\n\" + \\\n",
    "                \"1. Why this instruction is not good? First analyse the instruction based on Complexity of the Topic, Level of Detail Required, Knowledge Required, Ambiguity of the Instruction and Logical Reasoning or Problem-Solving Involved. \\n\" + \\\n",
    "                \"Then analyse why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"Finally analyse why this bad instruction lead to a bad answer. \" +\\\n",
    "                \"2. Based on the reason you provided, generate a new and complete instruction which is complex and difficult to answer directly. \" + \\\n",
    "                \"Make sure the new instruction is relevent but independent to the original instruction, which can be answered without knowing the original instruction, put the new instruction in the format of [New Instruction] your instruction [End]\" +\\\n",
    "                \"3. Answer the newly generated instruction as detailed as possible, in the format of [New Answer] your answer [End] \\n\"\n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To see how it works, consider the dataset entry, json_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pprint(json_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can refine the instruction as follows, using instr_prompt_no_input function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "entry = json_data[2]\n",
    "\n",
    "system_prompt, prompt = instr_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The response is very verbose, which is useful for analysis purposes; also, it helps the GPT-4 model to make improvements via the chain-of-thought prompting approach\n",
    "###### However, to construct the improved dataset, we are actually only interested in new instructions and outputs, not the analyses\n",
    "###### We can use the following utility code from the Reflection-Tuning repository to extract the model's improved instructions and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ins(text, no_input=True):\n",
    "    if '[New Instruction]' in text:\n",
    "        pattern = r'(\\[New Instruction\\])(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    else:\n",
    "        pattern = r'(New Instruction:)(.*?)(\\[End\\]|\\[New Answer\\]|New Answer:)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_ins = ''\n",
    "    else:\n",
    "        seg_ins = segments[0][1].strip()\n",
    "    if seg_ins.endswith(\"\\n\\n3.\"):\n",
    "        seg_ins = seg_ins[:-4]\n",
    "    return seg_ins\n",
    "\n",
    "\n",
    "def extract_oup(text, no_input=True):\n",
    "    if '[New Answer]' in text:\n",
    "        pattern = r'(\\[New Answer\\])(.*?)(\\[End\\]|$)'\n",
    "    else:\n",
    "        pattern = r'(New Answer:)(.*?)(\\[End\\]|$)'\n",
    "        # pattern = r'(\\[New Answer\\]|New Answer:)(.*?)(\\[End\\]|$)'\n",
    "    segments = re.findall(pattern, text, re.DOTALL)\n",
    "    if len(segments) == 0:\n",
    "        seg_oup = ''\n",
    "    else:\n",
    "        seg_oup = segments[0][1].strip()\n",
    "    return seg_oup\n",
    "\n",
    "\n",
    "def extract_instruction(text):\n",
    "    if text == '':\n",
    "        return []\n",
    "    seg_ins = extract_ins(text, no_input=True)\n",
    "    seg_oup = extract_oup(text, no_input=True)\n",
    "    return [seg_ins, seg_oup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's use these utility functions to extract the improved instruction and response from the lengthy GPT-4 output generated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "new_instr, new_outp = extract_instruction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(new_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(new_outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note that the instruction-refinement is currently only implemented for dataset entries that don't have an \"input\" field\n",
    "#### Improve Responses\n",
    "###### In a similar fashion, we can also apply the Reflection-Tuning refinement process specifically to the dataset responses (i.e., \"output\" fields)\n",
    "###### Below are two small utility functions from the Reflection-Tuning repository to format the inputs to the GPT-4 model for dataset refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def res_gen_prompt_no_input(ins, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful, precise but picky assistant for checking the quality of the answer to a given instruction.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt\n",
    "\n",
    "\n",
    "def res_gen_prompt_input(ins, inp, outp):\n",
    "\n",
    "    sys_prompt = \"You are a helpful and precise assistant for checking the quality of the answer to a given instruction and its input.\"\n",
    "    prompt_template = \"[Instruction]\\n{ins}\\n\\n[The Start of Input]\\n{inp}\\n\\n[The End of Input]\\n\\n[The Start of Answer]\\n{outp}\\n\\n[The End of Answer]\\n\\n[System]\\n{criteria}\\n\\n\"\n",
    "    criteria = \"We would like you to answer several questions related to the quality of the answer to the given instruction and corresponding input. \\n\" + \\\n",
    "                \"1. Why this answer is not good for the given instruction and corresponding input? Analyse based on the Helpfulness, Relevance, Accuracy and Level of Details. \\n\" + \\\n",
    "                \"2. Based on the reason you provided, generate a better answer, new and complete, as detailed as possible, in the format of [Better Answer] your answer [End] \\n\" \n",
    "    prompt = prompt_template.format(\n",
    "        ins=ins, inp=inp, outp=outp, criteria=criteria\n",
    "    )\n",
    "    return sys_prompt, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Again, let's apply it to one of the dataset entries to see how it works, generating the improved response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "entry = json_data[2]\n",
    "\n",
    "system_prompt, prompt = res_gen_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see above, the response includes an analysis of the original response; we can extract the new response using the following utility function from the Reflection-Tuning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_response(text):\n",
    "    if text.count('[Better Answer]') >= 2:\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|\\[Better Answer\\]|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    else:\n",
    "        # pattern = r'\\[(Better Answer)\\](.*?)\\[End\\]'\n",
    "        pattern = r'\\[(Better Answer)\\](.*?)(\\[End\\]|End|$)'\n",
    "        segments = re.findall(pattern, text, re.DOTALL)\n",
    "    return [segment[1].strip() for segment in segments]\n",
    "response = extract_response(output)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the Dataset\n",
    "###### Now, let's apply the instruction-reflection and response-reflection techniques to the actual dataset\n",
    "###### Note: we only apply it to a small data subset here for demo purposes; to apply it to the whole dataset, change\n",
    "###### data_to_process = json_data[:3]\n",
    "###### to\n",
    "\n",
    "###### data_to_process = json_data\n",
    "###### Reflect instructions\n",
    "###### The following code applies the Reflection-Tuning methodology for dataset refinement to the instructions in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reflect_instructions(json_data, client):\n",
    "    new_json_data = [] \n",
    "    \n",
    "    for entry in tqdm(json_data):\n",
    "        \n",
    "        if not entry[\"input\"]:\n",
    "            system_prompt, prompt = instr_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_instr, new_outp = extract_instruction(output)\n",
    "            new_entry = {\"instruction\": new_instr, \"input\": \"\", \"output\": new_outp}\n",
    "            new_json_data.append(new_entry)\n",
    "        else:\n",
    "            new_json_data.append(entry)\n",
    "\n",
    "    return new_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]\n",
    "\n",
    "new_json_data = reflect_instructions(data_to_process, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i in new_json_data[:3]:\n",
    "    pprint(i)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's save the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"instruction-reflected.json\", \"w\") as file:\n",
    "    json.dump(new_json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflect responses\n",
    "###### Let's now do the same for the response-reflection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_to_process = json_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def reflect_responses(json_data, client):\n",
    "    new_json_data = [] \n",
    "    \n",
    "    for entry in tqdm(json_data):\n",
    "        \n",
    "        if not entry[\"input\"]:\n",
    "            system_prompt, prompt = res_gen_prompt_no_input(ins=entry[\"instruction\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_response = extract_response(output)\n",
    "\n",
    "            if not len(new_response):\n",
    "                new_response = entry[\"output\"]\n",
    "                      \n",
    "            new_entry = {\"instruction\": entry[\"instruction\"], \"input\": \"\", \"output\": new_response[0]}\n",
    "            new_json_data.append(new_entry)\n",
    "\n",
    "        else:\n",
    "            system_prompt, prompt = res_gen_prompt_input(ins=entry[\"instruction\"], inp=entry[\"input\"], outp=entry[\"output\"])\n",
    "            output = run_chatgpt(prompt=prompt, client=client, system_prompt=system_prompt)\n",
    "            new_response = extract_response(output)\n",
    "\n",
    "            if not len(new_response):\n",
    "                new_response = entry[\"output\"]\n",
    "\n",
    "            new_entry = {\"instruction\": entry[\"instruction\"], \"input\": entry[\"input\"], \"output\": new_response[0]}\n",
    "            new_json_data.append(new_entry)\n",
    "\n",
    "    return new_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "new_json_data = reflect_responses(data_to_process, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i in new_json_data[:3]:\n",
    "    pprint(i)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's save the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"response-reflected.json\", \"w\") as file:\n",
    "    json.dump(new_json_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Improved Instruction Data\n",
    "###### Applying the two methodologies above to all 1100 entries in the chapter 7 instruction dataset costs about $0.60 (60 cents)\n",
    "###### To avoid bloating the GitHub repository with dataset files, the resulting dataset files are available from Google Drive:\n",
    "###### instruction-reflected.json\n",
    "###### response-reflected.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
