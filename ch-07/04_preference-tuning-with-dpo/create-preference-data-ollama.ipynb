{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generating A Preference Dataset With Llama 3.1 70B And Ollama\n",
    "###### Preference finetuning is a process to align an instruction-finetuned LLM with human preferences\n",
    "###### There are multiple ways to create a dataset for preference finetuning an LLM\n",
    "###### We use the instruction-finetuned LLM to generate multiple responses and have humans rank them based on ###### their preference and/or given preference criteria\n",
    "###### We use the instruction-finetuned LLM to generate multiple responses and have LLMs rank them based on ###### given preference criteria\n",
    "###### We use an LLM to generate preferred and dispreferred responses given certain preference criteria\n",
    "###### In this notebook, we consider approach 3\n",
    "###### This notebook uses a 70-billion-parameter Llama 3.1-Instruct model through ollama to generate preference ###### labels for an instruction dataset\n",
    "###### The expected format of the instruction dataset is as follows:\n",
    "###### Input\n",
    "###### \n",
    "###### [\n",
    "######     {\n",
    "######         \"instruction\": \"What is the state capital of California?\",\n",
    "######         \"input\": \"\",\n",
    "######         \"output\": \"The state capital of California is Sacramento.\",\n",
    "######     },\n",
    "######     {\n",
    "######         \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "######         \"input\": \"\",\n",
    "######         \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "######     },\n",
    "######     {\n",
    "######         \"instruction\": \"What is the capital of Greece?\",\n",
    "######         \"input\": \"\",\n",
    "######         \"output\": \"The capital of Greece is Athens.\",\n",
    "###### \n",
    "######     },\n",
    "###### ...\n",
    "###### ]\n",
    "###### The output dataset will look as follows, where more polite responses are preferred ('chosen'), and more impolite responses are dispreferred ('rejected'):\n",
    "\n",
    "###### [\n",
    "######     {\n",
    "######         \"instruction\": \"What is the state capital of California?\",\n",
    "######         \"input\": \"\",\n",
    "######         \"output\": \"The state capital of California is Sacramento.\",\n",
    "######         \"rejected\": \"Look, the state capital of California is obviously Sacramento.\",\n",
    "######         \"chosen\": \"The state capital of California is Sacramento.\"\n",
    "######    },\n",
    "######    {\n",
    "######        \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "######        \"input\": \"\",\n",
    "######        \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "######        \"chosen\": \"A suitable alternative to 'fast' would be 'quick'.\",\n",
    "######        \"rejected\": \"A synonym for 'fast' is 'quick'.\"\n",
    "######    },\n",
    "######    {\n",
    "######         \"instruction\": \"What is the capital of Greece?\",\n",
    "######         \"input\": \"\",\n",
    "######         \"output\": \"The capital of Greece is Athens.\",\n",
    "######         \"chosen\": \"I'd be happy to help! The capital of Greece is indeed Athens.\",\n",
    "######         \"rejected\": \"The capital of Greece is Athens.\"\n",
    "######     },\n",
    "###### ...\n",
    "###### ]\n",
    "\n",
    "#### Output\n",
    "\n",
    "###### The code doesn't require a GPU and runs on a laptop given enough RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # Progress bar\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Ollama and Downloading Llama 3.1\n",
    "###### Ollama is an application to run LLMs efficiently\n",
    "###### It is a wrapper around llama.cpp, which implements LLMs in pure C/C++ to maximize efficiency\n",
    "###### Note that it is a tool for using LLMs to generate text (inference), not training or finetuning LLMs\n",
    "###### Prior to running the code below, install ollama by visiting https://ollama.com and following the ###### instructions (for instance, clicking on the \"Download\" button and downloading the ollama application for ###### your operating system)\n",
    "###### For macOS and Windows users, click on the ollama application you downloaded; if it prompts you to install ###### the command line usage, say \"yes\"\n",
    "###### \n",
    "###### Linux users can use the installation command provided on the ollama website\n",
    "###### \n",
    "###### In general, before we can use ollama from the command line, we have to either start the ollama ###### application or run ollama serve in a separate terminal\n",
    "\n",
    "###### With the ollama application or ollama serve running, in a different terminal, on the command line, execute the following command to try out the 70-billion-parameter Llama 3.1 model\n",
    "\n",
    "# 70B model\n",
    "###### ollama run llama3.1:70b\n",
    "###### The output looks like as follows:\n",
    "###### \n",
    "###### $ ollama run llama3.1:70b\n",
    "###### pulling manifest\n",
    "###### pulling aa81b541aae6... 100% ▕████████████████▏ 39 GB\n",
    "###### pulling 8cf247399e57... 100% ▕████████████████▏ 1.7 KB\n",
    "###### pulling f1cd752815fc... 100% ▕████████████████▏ 12 KB\n",
    "###### pulling 56bb8bd477a5... 100% ▕████████████████▏ 96 B\n",
    "###### pulling 3c1c2d3df5b3... 100% ▕████████████████▏ 486 B\n",
    "###### verifying sha256 digest\n",
    "###### writing manifest\n",
    "###### removing any unused layers\n",
    "###### success\n",
    "###### Note that llama3.1:70b refers to the instruction finetuned 70-billion-parameter Llama 3.1 model\n",
    "\n",
    "###### Alternatively, you can also use the smaller, more resource-effiicent 8-billion-parameters Llama 3.1 ###### model, by replacing llama3.1:70b with llama3.1\n",
    "\n",
    "###### After the download has been completed, you will see a command line prompt that allows you to chat with ###### the model\n",
    "\n",
    "###### Try a prompt like \"What do llamas eat?\", which should return an output similar to the following:\n",
    "\n",
    "###### >>> What do llamas eat?\n",
    "###### Llamas are ruminant animals, which means they have a four-chambered \n",
    "###### stomach and eat plants that are high in fiber. In the wild, llamas \n",
    "###### typically feed on:\n",
    "###### 1. Grasses: They love to graze on various types of grasses, including tall \n",
    "###### grasses, wheat, oats, and barley.\n",
    "###### You can end this session using the input /bye\n",
    "###### Using Ollama's REST API\n",
    "###### Now, an alternative way to interact with the model is via its REST API in Python via the following ###### function\n",
    "###### Before you run the next cells in this notebook, make sure that ollama is still running, as described ###### above, via\n",
    "###### ollama serve in a terminal\n",
    "###### the ollama application\n",
    "###### Next, run the following code cell to query the model\n",
    "###### First, let's try the API with a simple example to make sure it works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def query_model(prompt, model=\"llama3.1:70b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON Entries\n",
    "###### Now, let's get to the data generation part\n",
    "###### Here, for a hands-on example, we use the instruction-data.json file that we originally used to instruction-finetune the model in chapter 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The structure of this file is as follows, where we have the given response in the test dataset ('output') that we trained the model to generate via instruction finetuning based on the 'input' and 'instruction'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below is a small utility function that formats the instruction and input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now, let's try the ollama API to generate a 'chosen' and 'rejected' response for preference tuning a model\n",
    "###### Here, to for illustration purposes, we create answers that are more or less polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for entry in json_data[:5]:\n",
    "    \n",
    "    politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return return the generated response and nothing else.\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(f\"\\n{politeness} response:\")\n",
    "    print(\">>\", query_model(prompt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If we find that the generated responses above look reasonable, we can go to the next step and apply the prompt to the whole dataset\n",
    "###### Here, we add a 'chosen' key for the preferred response and a 'rejected' response for the dispreferred response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_responses(json_data):\n",
    "\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entries\")):\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return return the generated response and nothing else.\"\n",
    "        )\n",
    "        response = query_model(prompt)\n",
    "        \n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's now apply this evaluation to the whole dataset and compute the average score of each model (this takes about 1 minute per model on an M3 MacBook Air laptop)\n",
    "###### Note that ollama is not fully deterministic across operating systems (as of this writing) so the numbers you are getting might slightly differ from the ones shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_model_responses(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
